---
title: "HW6 Telemarketing"
author: "Jack Yu, Max Nolan, Gloria Stach, Zihan Zeng, Weiqing Li"
date: "3/22/2020"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
```

## Getting Data Ready for Analysis

```{r, cache=TRUE}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))

str(telemm)
```

## Clustering Model

```{r}
# Set seed for uniform model result
set.seed(12345)
# Remove yyes so model not learning it
tele_clustering <- tele_norm[-53]
tele_cluster_z <- as.data.frame(lapply(tele_clustering, scale))
# Train model with 5 clusters
tele_clusters <- kmeans(tele_cluster_z, 5)
# Create variable to store cluster group
tele_cluster_z$cluster <- tele_clusters$cluster
tele_cluster_z$yyes <- tele_norm$yyes
tele_clusters$centers
tele_clusters$size
# Evaluate cluster prediction
aggregate(data = tele_cluster_z, yyes ~ cluster, mean)
# Now we separate out each group
group_1 <- tele_cluster_z[tele_cluster_z$cluster == 1,]
group_2 <- tele_cluster_z[tele_cluster_z$cluster == 2,]
group_5 <- tele_cluster_z[tele_cluster_z$cluster == 5,]
index_3 <- which(tele_cluster_z$cluster == 3)
index_4 <- which(tele_cluster_z$cluster == 4)
# Create new column called "call" to store our decision
tele$call <- "NO"
# Make group 3 and 4 YES indicating we will call them
tele[index_3,]$call <- "YES"
tele[index_4,]$call <- "YES"

cluster_data = list(group_1,group_2,group_5)
```

From the output, we can see that for cluster 3 and 4 the yyes numbers are large so we will call all of them. TODO: write more about how we store the data. Now we run three different models on each of cluster 1, 2 and 5. And use majority voting to decide if we are going to make the call.

## Logistic Regression

```{r}
# Select 3000 random rows for group 1 test data
test_set1 <- sample(1:nrow(group_1), 3000)

# Create the train and test set
group1_train <- group_1[-test_set1, ]
group1_test <- group_1[test_set1, ]

# Logistic regression model for group 1 with all predictor variables
lrmodel_1 <- glm(yyes ~. -cluster, data = group1_train, family = "binomial")
summary(lrmodel_1)

# Dropping predictor variables that has strong collinearity
lrmodel_1 <- glm(yyes ~. - jobunknown - educationilliterate - defaultyes - loanunknown - contacttelephone - monthmar - monthsep - monthdec - monthnov - monthoct - poutcomenonexistent - poutcomesuccess - cons.conf.idx - nr.employed - pdaysdummy - cluster - emp.var.rate - cons.price.idx - previous, data = group1_train, family = "binomial")
summary(lrmodel_1)

# Predict outcomes using the model
lroutcome_1 <- predict(lrmodel_1, newdata = group1_test, type = "response")

# Convert response into binary outcome
lroutcome_1 <- ifelse(lroutcome_1 <= 0.5, 0, 1)

# Evaluate the prediction
library(gmodels)
library(caret)
CrossTable(x = group1_test$yyes, y = lroutcome_1, prop.chisq=FALSE)
confusionMatrix(as.factor(group1_test$yyes), as.factor(lroutcome_1), positive = "1")
```

For group 1, the result of the logistic regression model predicts a total of 7 buying customers that we should call. Among them, 6 customers are actually making the purchase. Therefore, if we follow the model's prediction, the success rate of our call would be its sensitivity, which is 6/7 = 0.86. 

```{r}
# Select 4000 random rows for group 2 test data
test_set2 <- sample(1:nrow(group_2), 4000)

# Create the train and test set
group2_train <- group_2[-test_set2, ]
group2_test <- group_2[test_set2, ]

# Logistic regression model for group 1 with all predictor variables
lrmodel_2 <- glm(yyes ~. - cluster, data = group2_train, family = "binomial")
summary(lrmodel_2)

# Dropping predictor variables that has strong collinearity
lrmodel_2 <- glm(yyes ~. - jobunknown - defaultyes - loanunknown - monthmar - monthsep - monthdec - monthoct - monthjun - monthmay - poutcomenonexistent - poutcomesuccess - cons.conf.idx - nr.employed - pdaysdummy - cluster - emp.var.rate - cons.price.idx, data = group2_train, family = "binomial")
summary(lrmodel_2)

# Predict outcomes using the model
lroutcome_2 <- predict(lrmodel_2, newdata = group2_test, type = "response")

# Convert response into binary outcome
lroutcome_2 <- ifelse(lroutcome_2 <= 0.5, 0, 1)

# Evaluate the prediction
library(gmodels)
library(caret)
CrossTable(x = group2_test$yyes, y = lroutcome_2, prop.chisq=FALSE)
confusionMatrix(as.factor(group2_test$yyes), as.factor(lroutcome_2), positive = "1")
```

For group 2, the result of the logistic regression model predicts a total of 3 buying customers that we should call. Among them, 2 customers are actually making the purchase. Therefore, if we follow the model's prediction, the success rate of our call would be its sensitivity, which is 2/3 = 0.67. 

```{r}
# Select 1000 random rows for group 5 test data
test_set5 <- sample(1:nrow(group_5), 1000)

# Create the train and test set
group5_train <- group_5[-test_set5, ]
group5_test <- group_5[test_set5, ]

# Logistic regression model for group 1 with all predictor variables
lrmodel_5 <- glm(yyes ~. - cluster, data = group5_train, family = "binomial")
summary(lrmodel_5)

# Dropping predictor variables that has strong collinearity
lrmodel_5 <- glm(yyes ~. - jobunemployed - jobunknown - educationilliterate - loanunknown - poutcomenonexistent - poutcomesuccess - pdaysdummy - cluster, data = group5_train, family = "binomial")
summary(lrmodel_5)

# Predict outcomes using the model
lroutcome_5 <- predict(lrmodel_5, newdata = group5_test, type = "response")

# Convert response into binary outcome
lroutcome_5 <- ifelse(lroutcome_5 <= 0.5, 0, 1)

# Evaluate the prediction
library(gmodels)
library(caret)
CrossTable(x = group5_test$yyes, y = lroutcome_5, prop.chisq=FALSE)
confusionMatrix(as.factor(group5_test$yyes), as.factor(lroutcome_5), positive = "1")
```

For group 5, the result of the logistic regression model predicts a total of 5 buying customers that we should call. Among them, 3 customers are actually making the purchase. Therefore, if we follow the model's prediction, the success rate of our call would be its sensitivity, which is 3/5 = 0.60. 



> Now you are ready to build your ANN model. Feel free to modify the data load, cleaning and preparation code above as per your preference.

## ANN model

```{r, cache=TRUE}
# Building a simple ANN model for group 1 prediction
library(neuralnet)
for(x in cluster_data){
  # generate random 3000 samples from group 1 as test data, rest as train data
  set.seed(430) # for reproducibility 
  yes_people = which(x$yyes == 1)
  no_people = which(x$yyes == 0)
  train_id = c(sample(yes_people , size = trunc(0.70 * length(yes_people ))),sample(no_people , size = trunc(0.70 * length(no_people ))))

  ANN_G1_train <- x[train_id,]
  ANN_G1_test <- x[-train_id,]
  # Train the ANN model with default 1 hidden layer
  # We don't train on cluster since it's all 1 for group_1
  ANN_mod_G1 <- neuralnet(formula = yyes ~ . - cluster, data = ANN_G1_train)
  plot(ANN_mod_G1)
  #evaluation of the model
  # Group 1 prediction with neural network with 1 neuron
  ANN_pred1 <- predict(ANN_mod_G1, newdata = ANN_G1_test)
  threshold <- 0.5
  predicted_yyes_1 <- ifelse(ANN_pred1 <= threshold, 0, 1)
  CrossTable(x = ANN_G1_test$yyes, y = predicted_yyes_1, prop.chisq=FALSE)
  confusionMatrix(as.factor(ANN_G1_test$yyes), as.factor(predicted_yyes_1), positive = "1")
  }
  # Now let's try with more hidden layers
  ANN_mod2_G1 <- neuralnet(yyes ~ . - cluster, data = ANN_G1_train, hidden = 3)
  plot(ANN_mod2_G1)
  ANN_pred2 <- predict(ANN_mod2_G1, newdata = ANN_G1_test)
  threshold <- 0.5
  predicted_yyes_2 <- ifelse(ANN_pred2 <= threshold, 0, 1)
  CrossTable(x = ANN_G1_test$yyes, y = predicted_yyes_2, prop.chisq=FALSE)
  confusionMatrix(as.factor(ANN_G1_test$yyes), as.factor(predicted_yyes_2), positive = "1")
  # With 5 layers
  ANN_mod3_G1 <- neuralnet(yyes ~ . - cluster, data = ANN_G1_train, hidden = 5)
  plot(ANN_mod3_G1)
  ANN_pred3 <- predict(ANN_mod3_G1, newdata = ANN_G1_test)
  threshold <- 0.5
  predicted_yyes_3 <- ifelse(ANN_pred3 <= threshold, 0, 1)
  CrossTable(x = ANN_G1_test$yyes, y = predicted_yyes_3, prop.chisq=FALSE)
  confusionMatrix(as.factor(ANN_G1_test$yyes), as.factor(predicted_yyes_3), positive = "1")

```
## ANN Model for Group

Thus we can see that the ANN model with 3 and 5 hidden layers didn't outperform the default model with 1 hidden layer for Group_1.

## KNN 

```{r}
set.seed(12345)
library(class)
library(caret)
library(gmodels)
  
for(x in cluster_data){
  #split into test and train
  yes_people = which(x$yyes == 1)
  no_people = which(x$yyes == 0)
  train_id = c(sample(yes_people , size = trunc(0.70 * length(yes_people ))),sample(no_people , size = trunc(0.70 * length(no_people ))))

  tele_train <- x[train_id, -match("yyes",names(x))]
  tele_test <- x[-train_id, -match("yyes",names(x))]
  
  #Now the response (aka Labels) - only the yyes column
  tele_train_labels <- x[train_id, "yyes"]
  tele_test_labels <- x[-train_id, "yyes"]
  

  #Run KNN on train data, create predictions for test data
  #Starting K value close to sqrt(nrow(wbcd_train))
  tele_test_pred <- knn(train = tele_train, test = tele_test,
                        cl = tele_train_labels, k=sqrt(nrow(tele_train)))
  
  #Evaluate model results
  CrossTable(x = tele_test_labels, y = tele_test_pred, prop.chisq=FALSE)
  confusionMatrix(as.factor(tele_test_pred), as.factor(tele_test_labels), positive = "1")
}
```

# Conclusion 

In order to determine whether kNN, LR, ANN, or a combined prediction model yields better results, we first need to discuss what "better results" is as well as note what we are keeping constant between the various models in order to ensure a fair comparision. 

## Defining Better Results

Our objective is to maximize the amount of money this firm generates by ensuring they call people who are more likely to buy. Since a singular call costs $1 and a sucsessful call, we wish to achieve a ratio where:

*  (numSucsessfulCalls / numTotalCalls) > 16.67% 

Notice however, that if all models achieve this metric, we wish to consider how many calls we are making as well. If a model can ensure calling 100 people will be profitable whereas another model can ensure that calling 1000 people will be profitable, we would need to determine the total gain on using that model vs the other according to:

* let totalProfit = $6 * numSucsessFulCalls * probCallIsSucsessful 

## Constants held throughout models 

In accordance with the aforementioned points, we wish to control for as many variables as possible to ensure that we are evaluating the models fairly. Therefore, we held all the models to the following standards and constraints:

* Fix the seed value to 12345 
* Train on 60% of our data set 
* Test on 40% of our data set 

# Results 

## K means clustering 

We have decided to cluster our potential customers into 5 groups. After testing the success rate of each group (if we call everyone in this group, will we achieve at least a 16.67% success rate?). 

We can clearly see that calling groups <FILL IN ONCE DATA IS PUSHED>. Therefore, if we call groups <FILL IN>, we will have a profit of:

* 

## Linear Regression







