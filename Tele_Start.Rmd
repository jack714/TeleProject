---
title: "HW6 Telemarketing"
author: "Jack Yu, Max Nolan, Gloria Stach, Zihan Zeng, Weiqing Li"
date: "3/22/2020"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
```

## Getting Data Ready for Analysis

```{r, cache=TRUE}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))

str(telemm)
```

## Clustering Model

```{r}
# Set seed for uniform model result
set.seed(12345)
# Remove yyes so model not learning it
tele_clustering <- tele_norm[-53]
tele_cluster_z <- as.data.frame(lapply(tele_clustering, scale))
# Train model with 5 clusters
tele_clusters <- kmeans(tele_cluster_z, 5)
# Create variable to store cluster group
tele_cluster_z$cluster <- tele_clusters$cluster
tele_cluster_z$yyes <- tele_norm$yyes
tele_clusters$centers
tele_clusters$size
# Evaluate cluster prediction
aggregate(data = tele_cluster_z, yyes ~ cluster, mean)
# Now we separate out each group
group_1 <- tele_cluster_z[tele_cluster_z$cluster == 1,]
group_2 <- tele_cluster_z[tele_cluster_z$cluster == 2,]
group_5 <- tele_cluster_z[tele_cluster_z$cluster == 5,]
index_3 <- which(tele_cluster_z$cluster == 3)
index_4 <- which(tele_cluster_z$cluster == 4)
group_1 <- tele_cluster_z[tele_cluster_z$cluster == 1,]
group_2 <- tele_cluster_z[tele_cluster_z$cluster == 2,]
group_3 <- tele_cluster_z[tele_cluster_z$cluster == 3,]
group_4 <- tele_cluster_z[tele_cluster_z$cluster == 4,]
group_5 <- tele_cluster_z[tele_cluster_z$cluster == 5,]
```

From the output, we can see that for cluster 3 and 4 the yyes numbers are large so we will call all of them. TODO: write more about how we store the data. Now we run three different models on each of cluster 1, 2 and 5. And use majority voting to decide if we are going to make the call.

## Logistic Regression

```{r}
# Select 3000 random rows for group 1 test data
test_set1 <- sample(1:nrow(group_1), 3000)

# Create the train and test set
group1_train <- group_1[-test_set1, ]
group1_test <- group_1[test_set1, ]

# Logistic regression model for group 1 with all predictor variables
lrmodel_1 <- glm(yyes ~. -cluster, data = group1_train, family = "binomial")
summary(lrmodel_1)

# Dropping predictor variables that has strong collinearity
lrmodel_1 <- glm(yyes ~. - jobunknown - educationilliterate - defaultyes - loanunknown - contacttelephone - monthmar - monthsep - monthdec - monthnov - monthoct - poutcomenonexistent - poutcomesuccess - cons.conf.idx - nr.employed - pdaysdummy - cluster - emp.var.rate - cons.price.idx - previous, data = group1_train, family = "binomial")
summary(lrmodel_1)

# Predict outcomes using the model
lroutcome_1 <- predict(lrmodel_1, newdata = group1_test, type = "response")

# Convert response into binary outcome
lroutcome_1 <- ifelse(lroutcome_1 <= 0.5, 0, 1)

# Evaluate the prediction
library(gmodels)
library(caret)
CrossTable(x = group1_test$yyes, y = lroutcome_1, prop.chisq=FALSE)
confusionMatrix(as.factor(group1_test$yyes), as.factor(lroutcome_1), positive = "1")
```

For group 1, the result of the logistic regression model predicts a total of 7 buying customers that we should call. Among them, 6 customers are actually making the purchase. Therefore, if we follow the model's prediction, the success rate of our call would be its sensitivity, which is 6/7 = 0.86. 

```{r}
# Select 4000 random rows for group 2 test data
test_set2 <- sample(1:nrow(group_2), 4000)

# Create the train and test set
group2_train <- group_2[-test_set2, ]
group2_test <- group_2[test_set2, ]

# Logistic regression model for group 1 with all predictor variables
lrmodel_2 <- glm(yyes ~. - cluster, data = group2_train, family = "binomial")
summary(lrmodel_2)

# Dropping predictor variables that has strong collinearity
lrmodel_2 <- glm(yyes ~. - jobunknown - defaultyes - loanunknown - monthmar - monthsep - monthdec - monthoct - monthjun - monthmay - poutcomenonexistent - poutcomesuccess - cons.conf.idx - nr.employed - pdaysdummy - cluster - emp.var.rate - cons.price.idx, data = group2_train, family = "binomial")
summary(lrmodel_2)

# Predict outcomes using the model
lroutcome_2 <- predict(lrmodel_2, newdata = group2_test, type = "response")

# Convert response into binary outcome
lroutcome_2 <- ifelse(lroutcome_2 <= 0.5, 0, 1)

# Evaluate the prediction
library(gmodels)
library(caret)
CrossTable(x = group2_test$yyes, y = lroutcome_2, prop.chisq=FALSE)
confusionMatrix(as.factor(group2_test$yyes), as.factor(lroutcome_2), positive = "1")
```

For group 2, the result of the logistic regression model predicts a total of 3 buying customers that we should call. Among them, 2 customers are actually making the purchase. Therefore, if we follow the model's prediction, the success rate of our call would be its sensitivity, which is 2/3 = 0.67. 

```{r}
# Select 1000 random rows for group 5 test data
test_set5 <- sample(1:nrow(group_5), 1000)

# Create the train and test set
group5_train <- group_5[-test_set5, ]
group5_test <- group_5[test_set5, ]

# Logistic regression model for group 1 with all predictor variables
lrmodel_5 <- glm(yyes ~. - cluster, data = group5_train, family = "binomial")
summary(lrmodel_5)

# Dropping predictor variables that has strong collinearity
lrmodel_5 <- glm(yyes ~. - jobunemployed - jobunknown - educationilliterate - loanunknown - poutcomenonexistent - poutcomesuccess - pdaysdummy - cluster, data = group5_train, family = "binomial")
summary(lrmodel_5)

# Predict outcomes using the model
lroutcome_5 <- predict(lrmodel_5, newdata = group5_test, type = "response")

# Convert response into binary outcome
lroutcome_5 <- ifelse(lroutcome_5 <= 0.5, 0, 1)

# Evaluate the prediction
library(gmodels)
library(caret)
CrossTable(x = group5_test$yyes, y = lroutcome_5, prop.chisq=FALSE)
confusionMatrix(as.factor(group5_test$yyes), as.factor(lroutcome_5), positive = "1")
```

For group 5, the result of the logistic regression model predicts a total of 5 buying customers that we should call. Among them, 3 customers are actually making the purchase. Therefore, if we follow the model's prediction, the success rate of our call would be its sensitivity, which is 3/5 = 0.60. 



> Now you are ready to build your ANN model. Feel free to modify the data load, cleaning and preparation code above as per your preference.

## ANN model

### Group 1
```{r, cache=TRUE}
# Building a simple ANN model for group 1 prediction
library(neuralnet)
# generate random 60% samples from group 1 as test data, rest as train data
yes_people_G1 = which(group_1$yyes == 1)
no_people_G1 = which(group_1$yyes == 0)
train_id_G1 = c(sample(yes_people_G1 , size = trunc(0.60 * length(yes_people_G1 ))),sample(no_people_G1 , size = trunc(0.60 * length(no_people_G1))))
ANN_G1_train <- group_1[-train_id_G1,]
ANN_G1_test <- group_1[train_id_G1,]
# Train the ANN model with default 1 hidden layer
# We don't train on cluster since it's all 1 for group_1
ANN_mod_G1 <- neuralnet(formula = yyes ~ . - cluster, data = ANN_G1_train)
plot(ANN_mod_G1)
# Evaluation of the model
# Group 1 prediction with neural network with 1 neuron
ANN_pred1 <- predict(ANN_mod_G1, newdata = ANN_G1_test)
# We set the threshold that will give the maximum revenue
threshold <- 0.6
predicted_yyes_1 <- ifelse(ANN_pred1 <= threshold, 0, 1)
CrossTable(x = ANN_G1_test$yyes, y = predicted_yyes_1, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_G1_test$yyes), as.factor(predicted_yyes_1), positive = "1")

# Now let's try with more hidden layers
ANN_mod2_G1 <- neuralnet(yyes ~ . - cluster, data = ANN_G1_train, hidden = 3)
plot(ANN_mod2_G1)
ANN_pred2 <- predict(ANN_mod2_G1, newdata = ANN_G1_test)
threshold <- 0.6
predicted_yyes_2 <- ifelse(ANN_pred2 <= threshold, 0, 1)
CrossTable(x = ANN_G1_test$yyes, y = predicted_yyes_2, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_G1_test$yyes), as.factor(predicted_yyes_2), positive = "1")
# With 5 layers
ANN_mod3_G1 <- neuralnet(yyes ~ . - cluster, data = ANN_G1_train, hidden = 5)
plot(ANN_mod3_G1)
ANN_pred3 <- predict(ANN_mod3_G1, newdata = ANN_G1_test)
threshold <- 0.6
predicted_yyes_3 <- ifelse(ANN_pred3 <= threshold, 0, 1)
CrossTable(x = ANN_G1_test$yyes, y = predicted_yyes_3, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_G1_test$yyes), as.factor(predicted_yyes_3), positive = "1")
group_1_ANN_decision <- predicted_yyes_3
```

Thus we can see that for Group 1, the ANN model with 1 hidden layer has profit of 17 * 6 - (47+17) * 1 = 38, the ANN model with 3 hidden layers has profit of 10 * 6 - (54+10) * 1 = -4, and the ANN model with 5 hidden layers has profit of 12 * 6 - (13+12) * 1 = 47. We thus choose the prediction results for the model with 5 hidden layers given its highest profit and the lower requirement on number of calls made.

### Group 2

```{r, cache=TRUE}
# Building a simple ANN model for group 2 prediction
# generate random 60% samples from group 2 as test data, rest as train data
yes_people_G2 = which(group_2$yyes == 1)
no_people_G2 = which(group_2$yyes == 0)
train_id_G2 = c(sample(yes_people_G2 , size = trunc(0.60 * length(yes_people_G2 ))),sample(no_people_G2 , size = trunc(0.60 * length(no_people_G2))))
ANN_G2_train <- group_2[-train_id_G2,]
ANN_G2_test <- group_2[train_id_G2,]
# Train the ANN model with default 1 hidden layer
# We don't train on cluster since it's all 1 for group_1
ANN_mod_G2 <- neuralnet(formula = yyes ~ . - cluster, data = ANN_G2_train)
plot(ANN_mod_G2)
# Evaluation of the model Group 2 prediction with 1 neuron
ANN_pred4 <- predict(ANN_mod_G2, newdata = ANN_G2_test)
# We set the threshold that will give the maximum revenue
threshold <- 0.13
predicted_yyes_4 <- ifelse(ANN_pred4 <= threshold, 0, 1)
CrossTable(x = ANN_G2_test$yyes, y = predicted_yyes_4, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_G2_test$yyes), as.factor(predicted_yyes_4), positive = "1")

# Now let's try with more hidden layers
ANN_mod2_G2 <- neuralnet(yyes ~ . - cluster, data = ANN_G2_train, hidden = 3)
plot(ANN_mod2_G2)
ANN_pred5 <- predict(ANN_mod2_G2, newdata = ANN_G2_test)
threshold <- 0.3
predicted_yyes_5 <- ifelse(ANN_pred5 <= threshold, 0, 1)
CrossTable(x = ANN_G2_test$yyes, y = predicted_yyes_5, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_G1_test$yyes), as.factor(predicted_yyes_2), positive = "1")
# With 5 layers
ANN_mod3_G2 <- neuralnet(yyes ~ . - cluster, data = ANN_G2_train, hidden = 5)
plot(ANN_mod3_G2)
ANN_pred6 <- predict(ANN_mod3_G2, newdata = ANN_G2_test)
threshold <- 0.85
predicted_yyes_6 <- ifelse(ANN_pred6 <= threshold, 0, 1)
CrossTable(x = ANN_G2_test$yyes, y = predicted_yyes_6, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_G1_test$yyes), as.factor(predicted_yyes_3), positive = "1")
group_2_ANN_decision <- predicted_yyes_6
```

We can see that for Group 2, the ANN model with 1 hidden layer has profit of 80 * 6 - (1175+80) * 1 = -775, the ANN model with 3 hidden layers has profit of 6 * 6 - (95+6) * 1 = -65, and the ANN model with 5 hidden layers has profit of 3 * 6 - (12+3) * 1 = 3. We thus choose the prediction results for the model with 5 hidden layers given its highest profit and the lower requirement on number of calls made. However, the profit is much lower than group 1's profit, indicating that group 2 may not be a good group to make calls.

### Model 5

```{r, cache=TRUE}
# Building a simple ANN model for group 5 prediction
# generate random 60% samples from group 5 as test data, rest as train data
yes_people_G5 = which(group_5$yyes == 1)
no_people_G5 = which(group_5$yyes == 0)
train_id_G5 = c(sample(yes_people_G5 , size = trunc(0.60 * length(yes_people_G5 ))),sample(no_people_G5 , size = trunc(0.60 * length(no_people_G5))))
ANN_G5_train <- group_2[-train_id_G5,]
ANN_G5_test <- group_2[train_id_G5,]
# Train the ANN model with default 1 hidden layer
# We don't train on cluster since it's all 1 for group_1
ANN_mod_G5 <- neuralnet(formula = yyes ~ . - cluster, data = ANN_G5_train)
plot(ANN_mod_G5)
# Evaluation of the model Group 2 prediction with 1 neuron
ANN_pred7 <- predict(ANN_mod_G5, newdata = ANN_G5_test)
# We set the threshold that will give the maximum revenue
threshold <- 0.05
predicted_yyes_7 <- ifelse(ANN_pred7 <= threshold, 0, 1)
CrossTable(x = ANN_G5_test$yyes, y = predicted_yyes_7, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_G5_test$yyes), as.factor(predicted_yyes_7), positive = "1")

# Now let's try with more hidden layers
ANN_mod2_G5 <- neuralnet(yyes ~ . - cluster, data = ANN_G5_train, hidden = 3)
plot(ANN_mod2_G5)
ANN_pred8 <- predict(ANN_mod2_G5, newdata = ANN_G5_test)
threshold <- 0.2
predicted_yyes_8 <- ifelse(ANN_pred8 <= threshold, 0, 1)
CrossTable(x = ANN_G5_test$yyes, y = predicted_yyes_8, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_G5_test$yyes), as.factor(predicted_yyes_8), positive = "1")
# With 5 layers
ANN_mod3_G5 <- neuralnet(yyes ~ . - cluster, data = ANN_G5_train, hidden = 5)
plot(ANN_mod3_G5)
ANN_pred9 <- predict(ANN_mod3_G5, newdata = ANN_G5_test)
threshold <- 0.37
predicted_yyes_9 <- ifelse(ANN_pred9 <= threshold, 0, 1)
CrossTable(x = ANN_G5_test$yyes, y = predicted_yyes_9, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_G5_test$yyes), as.factor(predicted_yyes_9), positive = "1")
group_5_ANN_decision <- predicted_yyes_9
```

We can see that for Group 5, the ANN model with 1 hidden layer has profit of 133 * 6 - (133+1937) * 1 = -1272, the ANN model with 3 hidden layers has profit of 3 * 6 - (38+3) * 1 = -23, and the ANN model with 5 hidden layers has profit of 1 * 6 - (1+1) * 1 = 4. We thus choose the prediction results for the model with 5 hidden layers given its highest profit. However, the profit is much lower than group 1's profit, indicating that group 5 may not be a good group to make calls.

## KNN 

## kNN prediction
```{r}
# Load the required library
library(caret)
library(tidyverse)
library(gmodels)
```

### Group 1 Analysis
```{r}
set.seed(12345)
# Selects random rows for the group 1 test data
yes_people1 = which(group_1$yyes == 1)
no_people1 = which(group_1$yyes == 0)
kNN_test_set1 = c(sample(yes_people1, size = trunc(0.40 * length(yes_people1 ))),sample(no_people1, size = trunc(0.40 * length(no_people1))))

# Create a train set and test set
#First the predictors - all columns except the yyes column
kNN_tele_train1 = group_1[kNN_test_set1, -match("yyes",names(group_1))]
kNN_tele_test1 = group_1[-kNN_test_set1, -match("yyes",names(group_1))]

# Now the response (aka Labels) - only the yyes column
kNN_tele_train_labels1 = group_1[kNN_test_set1, "yyes"]
kNN_tele_test_labels1 = group_1[-kNN_test_set1, "yyes"]
```

```{r}
set.seed(12345)
# Pick a reasonable K value from 1 to 15
grid1 = expand.grid(k = seq(1, 15, by = 1))
control1 = trainControl(method = "cv")
train1 = kNN_tele_test1 %>% cbind(kNN_tele_test_labels1) %>% rename(class = kNN_tele_test_labels1)
knn.train1 = train(class~., data = train1, method = "knn", trControl = control1, tuneGrid = grid1)
knn.train1
```

```{r}
set.seed(12345)
# Summarizing the results above and trying them, we choose K = 4
kNN_test_pred1 = knn(train = kNN_tele_train1, test = kNN_tele_test1,
                      cl = kNN_tele_train_labels1, 4)
#Evaluate model results
CrossTable(x = kNN_tele_test_labels1, y = kNN_test_pred1, 
           prop.chisq=FALSE)
```

### Group 2 Analysis
```{r}
set.seed(12345)
# Selects random rows for the group 2 test data
yes_people2 = which(group_2$yyes == 1)
no_people2 = which(group_2$yyes == 0)
kNN_test_set2 = c(sample(yes_people2, size = trunc(0.40 * length(yes_people2 ))),sample(no_people2, size = trunc(0.40 * length(no_people2))))

# Create a train set and test set
#First the predictors - all columns except the yyes column
kNN_tele_train2 = group_2[kNN_test_set2, -match("yyes",names(group_2))]
kNN_tele_test2 = group_2[-kNN_test_set2, -match("yyes",names(group_2))]

# Now the response (aka Labels) - only the yyes column
kNN_tele_train_labels2 = group_2[kNN_test_set2, "yyes"]
kNN_tele_test_labels2 = group_2[-kNN_test_set2, "yyes"]
```

```{r}
set.seed(12345)
# Pick a reasonable K value from 1 to 15
grid2 = expand.grid(k = seq(1, 15, by = 1))
control2 = trainControl(method = "cv")
train2 = kNN_tele_test2 %>% cbind(kNN_tele_test_labels2) %>% rename(class = kNN_tele_test_labels2)
knn.train2 = train(class~., data = train2, method = "knn", trControl = control2, tuneGrid = grid2)
knn.train2
```

```{r}
set.seed(12345)
# Summarizing the results above and trying them, we choose K = 5
kNN_test_pred2 = knn(train = kNN_tele_train2, test = kNN_tele_test2,
                      cl = kNN_tele_train_labels2, 5)
#Evaluate model results
CrossTable(x = kNN_tele_test_labels2, y = kNN_test_pred2, 
           prop.chisq=FALSE)
```

### Group 5 Analysis
```{r}
set.seed(12345)
# Selects random rows for the group 5 test data
yes_people5 = which(group_5$yyes == 1)
no_people5 = which(group_5$yyes == 0)
kNN_test_set5 = c(sample(yes_people5, size = trunc(0.40 * length(yes_people5 ))),sample(no_people5, size = trunc(0.40 * length(no_people5))))

# Create a train set and test set
#First the predictors - all columns except the yyes column
kNN_tele_train5 = group_5[kNN_test_set5, -match("yyes",names(group_5))]
kNN_tele_test5 = group_5[-kNN_test_set5, -match("yyes",names(group_5))]

# Now the response (aka Labels) - only the yyes column
kNN_tele_train_labels5 = group_5[kNN_test_set5, "yyes"]
kNN_tele_test_labels5 = group_5[-kNN_test_set5, "yyes"]
```

```{r}
set.seed(12345)
# Pick a reasonable K value from 1 to 15
grid5 = expand.grid(k = seq(1, 15, by = 1))
control5 = trainControl(method = "cv")
train5 = kNN_tele_test5 %>% cbind(kNN_tele_test_labels5) %>% rename(class = kNN_tele_test_labels5)
knn.train5 = train(class~., data = train5, method = "knn", trControl = control5, tuneGrid = grid5)
knn.train5
```

```{r}
set.seed(12345)
# Summarizing the result above and trying them, we choose K = 4
kNN_test_pred5 = knn(train = kNN_tele_train5, test = kNN_tele_test5,
                      cl = kNN_tele_train_labels5, 4)

#Evaluate model results
CrossTable(x = kNN_tele_test_labels5, y = kNN_test_pred5, 
           prop.chisq=FALSE)
```

**kNN part conclusion**
For the group 1, we get a profit of (15 * 6) - (38 + 15) * 1 = 37; For the group 2, we get a profit of (0 * 6) - (15 + 0) * 1 = -15, which means we will not call this group; For the group 5, we get a profit of (12 * 6) - (31 + 12) * 1 = 29. Since under the kNN model prediction, only the group 1 and 5 are profitable, only these 2 groups should be called. However, the group 5 has lower profit than the group 1. The company should consider carefully if it want to call the group.

# Conclusion 

In order to determine whether kNN, LR, ANN, or a combined prediction model yields better results, we first need to discuss what "better results" is as well as note what we are keeping constant between the various models in order to ensure a fair comparision. 

## Defining Better Results

Our objective is to maximize the amount of money this firm generates by ensuring they call people who are more likely to buy. Since a singular call costs $1 and a sucsessful call, we wish to achieve a ratio where:

*  (numSucsessfulCalls / numTotalCalls) > 16.67% 

Notice however, that if all models achieve this metric, we wish to consider how many calls we are making as well. If a model can ensure calling 100 people will be profitable whereas another model can ensure that calling 1000 people will be profitable, we would need to determine the total gain on using that model vs the other according to:

* let totalProfit = $6 * numSucsessFulCalls * probCallIsSucsessful 

## Constants held throughout models 

In accordance with the aforementioned points, we wish to control for as many variables as possible to ensure that we are evaluating the models fairly. Therefore, we held all the models to the following standards and constraints:

* Fix the seed value to 12345 
* Train on 60% of our data set 
* Test on 40% of our data set 

# Results 

## K means clustering 

We have decided to cluster our potential customers into 5 groups. After testing the success rate of each group (if we call everyone in this group, will we achieve at least a 16.67% success rate?). 

We can clearly see that calling groups <FILL IN ONCE DATA IS PUSHED>. Therefore, if we call groups <FILL IN>, we will have a profit of:

* 

## Linear Regression







