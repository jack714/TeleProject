---
title: "HW6 Telemarketing"
author: "Jack Yu, Max Nolan, Gloria Stach, Zihan Zeng, Weiqing Li"
date: "3/22/2020"
output:
  html_document:
    toc: true
    theme: readable
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Downloading and Prepping the Data

```{r}
#Downloading and Prepping the Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)
```

## Getting Data Ready for Analysis

```{r, cache=TRUE}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into knn has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

#Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# we are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))

str(telemm)
```

## Clustering Model

```{r}
# Set seed for uniform model result
set.seed(12345)
# Remove yyes so model not learning it
tele_clustering <- tele_norm[-53]
tele_cluster_z <- as.data.frame(lapply(tele_clustering, scale))
# Train model with 5 clusters
tele_clusters <- kmeans(tele_cluster_z, 5)
# Create variable to store cluster group
tele_cluster_z$cluster <- tele_clusters$cluster
tele_cluster_z$yyes <- tele_norm$yyes
tele_clusters$centers
tele_clusters$size
# Evaluate cluster prediction
aggregate(data = tele_cluster_z, yyes ~ cluster, mean)
# Now we separate out each group
group_1 <- tele_cluster_z[tele_cluster_z$cluster == 1,]
group_2 <- tele_cluster_z[tele_cluster_z$cluster == 2,]
group_5 <- tele_cluster_z[tele_cluster_z$cluster == 5,]
index_3 <- which(tele_cluster_z$cluster == 3)
index_4 <- which(tele_cluster_z$cluster == 4)
# Create new column called "call" to store our decision
tele$call <- "NO"
# Make group 3 and 4 YES indicating we will call them
tele[index_3,]$call <- "YES"
tele[index_4,]$call <- "YES"
```

From the output, we can see that for cluster 3 and 4 the yyes numbers are large so we will call all of them. TODO: write more about how we store the data. Now we run three different models on each of cluster 1, 2 and 5. And use majority voting to decide if we are going to make the call.

## Logistic Regression

```{r}
# Select 3000 random rows for group 1 test data
test_set1 <- sample(1:nrow(group_1), 3000)

# Create the train and test set
group1_train <- group_1[-test_set1, ]
group1_test <- group_1[test_set1, ]

# Logistic regression model for group 1 with all predictor variables
lrmodel_1 <- glm(yyes ~. -cluster, data = group1_train, family = "binomial")
summary(lrmodel_1)

# Dropping predictor variables that has strong collinearity
lrmodel_1 <- glm(yyes ~. - jobunknown - defaultyes - loanunknown - contacttelephone - monthmar - monthsep - monthdec - monthnov - monthoct - poutcomenonexistent - poutcomesuccess - cons.conf.idx - nr.employed - pdaysdummy - cluster - emp.var.rate - cons.price.idx, data = group_1, family = "binomial")
summary(lrmodel_1)

# Predict outcomes using the model
lroutcome_1 <- predict(lrmodel_1, newdata = group1_test, type = "response")

# Convert response into binary outcome
lroutcome_1 <- ifelse(lroutcome_1 <= 0.5, 0, 1)

# Evaluate the prediction
library(gmodels)
library(caret)
CrossTable(x = group1_test$yyes, y = lroutcome_1, prop.chisq=FALSE)
confusionMatrix(as.factor(group1_test$yyes), as.factor(lroutcome_1), positive = "1")
```

The result of the logistic regression model predicts a total of 12 buying customers that we should call. Among them, 11 customers are actually making the purchase. Therefore, if we follow the model's prediction, the success rate of our call would be its sensitivity, which is 11/12 = 0.92. 

## Getting Train and Test Samples for KNN

```{r}
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(tele_norm), 10000) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

# Create a train set and test set
#First the predictors - all columns except the yyes column
tele_train <- tele_norm[-test_set, -match("yyes",names(tele_norm))]
tele_test <- tele_norm[test_set, -match("yyes",names(tele_norm))]

#Now the response (aka Labels) - only the yyes column
tele_train_labels <- tele_norm[-test_set, "yyes"]
tele_test_labels <- tele_norm[test_set, "yyes"]
```


## ANN model

```{r, cache=TRUE}
# Building a simple ANN model for group 1 prediction
library(neuralnet)
# generate random 3000 samples from group 1 as test data, rest as train data
test_index <- sample(1:nrow(group_1), 3000)
ANN_G1_train <- group_1[-test_index,]
ANN_G1_test <- group_1[test_index,]
# Train the ANN model with default 1 hidden layer
# We don't train on cluster since it's all 1 for group_1
ANN_mod_G1 <- neuralnet(formula = yyes ~ . - cluster, data = ANN_G1_train)
plot(ANN_mod_G1)
```

## Evaluate ANN Model

```{r, cache=TRUE}
# Group 1 prediction with neural network with 1 neuron
ANN_pred1 <- predict(ANN_mod_G1, newdata = ANN_G1_test)
threshold <- 0.5
predicted_yyes_1 <- ifelse(ANN_pred1 <= threshold, 0, 1)
CrossTable(x = ANN_G1_test$yyes, y = predicted_yyes_1, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_G1_test$yyes), as.factor(predicted_yyes_1), positive = "1")
# Now let's try with more hidden layers
ANN_mod2_G1 <- neuralnet(yyes ~ . - cluster, data = ANN_G1_train, hidden = 3)
plot(ANN_mod2_G1)
ANN_pred2 <- predict(ANN_mod2_G1, newdata = ANN_G1_test)
threshold <- 0.5
predicted_yyes_2 <- ifelse(ANN_pred2 <= threshold, 0, 1)
CrossTable(x = ANN_G1_test$yyes, y = predicted_yyes_2, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_G1_test$yyes), as.factor(predicted_yyes_2), positive = "1")
# With 5 layers
ANN_mod3_G1 <- neuralnet(yyes ~ . - cluster, data = ANN_G1_train, hidden = 5)
plot(ANN_mod3_G1)
ANN_pred3 <- predict(ANN_mod3_G1, newdata = ANN_G1_test)
threshold <- 0.5
predicted_yyes_3 <- ifelse(ANN_pred3 <= threshold, 0, 1)
CrossTable(x = ANN_G1_test$yyes, y = predicted_yyes_3, prop.chisq=FALSE)
confusionMatrix(as.factor(ANN_G1_test$yyes), as.factor(predicted_yyes_3), positive = "1")
```

Thus we can see that the ANN model with 3 and 5 hidden layers didn't outperform the default model with 1 hidden layer for Group_1.

#Perform KNN model
```{r}
library(class)
library(caret)

#Run KNN on train data, create predictions for test data
#Starting K value close to sqrt(nrow(wbcd_train))
tele_test_pred <- knn(train = tele_train, test = tele_test,
                      cl = tele_train_labels, k=sqrt(nrow(tele_train)))

#Evaluate model results
library(gmodels)
CrossTable(x = tele_test_labels, y = tele_test_pred, 
           prop.chisq=FALSE)

confusionMatrix(as.factor(tele_test_pred), as.factor(tele_test_labels), positive = "1")
```

